"""nusacrowd zero-shot prompt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ru8DyS2ALWfRdkjOPHj-KNjw6Pfa44Nd
"""
import os, sys
sys.path.append(os.path.join(os.path.dirname(__file__), "src"))

import csv
from os.path import exists
import glob
import random

import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import f1_score, accuracy_score, classification_report

import torch
import torch.nn.functional as F
import datasets

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM

from dataset_utils import load_dataset
from indexer import SimpleDatasetIndexer, XpressoDatasetIndexer
from prompter import ICLPrompter, ITCPrompter
from classifier import predict_classification_batch
from transformers import pipeline

DEBUG=False

lang_map = {
    'btk': 'Batak', 'sun': 'Sundanese', 'jav': 'Javanese', 
    'mad': 'Madurese', 'mak': 'Buginese', 'min': 'Minangkabau',
    'amh': 'Amharic', 'hau': 'Hausa', 'ibo': 'Igbo', 'lug': 'Luganda', 'pcm': 'Nigerian Pidgin',
    'sna': 'chShona', 'swa': 'Kiswahili', 'xho': 'isiXhosa', 'yor': 'Yorùbá',
    'aym': 'Aymara', 'bzd': 'Bribri', 'cni': 'Asháninka', 'gn': 'Guaraní', 'hch': 'Wixarika',
    'nah': 'Nahuatl', 'oto': 'Otomí', 'quy': 'Quechua', 'shp': 'Shipibo-Konibo', 'tar': 'Rarámuri',
    'ind': 'Indonesian', 'eng': 'English', 'spa': 'Spanish', 'arb': 'Arabic', 'fra': 'French', 
    'deu': 'German', 'hin': 'Hindi', 'ita': 'Italian', 'por': 'Portuguese'
}

bcp47_lang_map = {
    'btk': None, 'sun': 'sun_Latn', 'jav': 'jav_Latn', 'mad': None, 
    'mak': 'bug_Latn', 'min': 'min_Arab', 'amh': 'amh_Ethi', 'hau': 'hau_Latn', 
    'ibo': 'ibo_Latn', 'lug': 'lug_Latn', 'pcm': 'eng_Latn', 'sna': 'sna_Latn',
    'swa': 'swh_Latn', 'xho': 'xho_Latn', 'yor': 'yor_Latn', 'aym': 'ayr_Latn', 
    'bzd': None, 'cni': None, 'gn': 'grn_Latn',  'hch': None, 'nah': None, 
    'oto': None, 'quy': 'quy_Latn', 'shp': None, 'tar': None, 'ind': 'ind_Latn', 
    'eng': 'eng_Latn', 'spa': 'spa_Latn', 'arb': 'arb_Latn', 'fra': 'fra_Latn',
    'deu': 'deu_Latn', 'hin': 'hin_Deva', 'ita': 'ita_Latn', 'por': 'por_Latn'
}

dataset_to_metadata_map = {
    # key: (prompt_template, icl_template, iia_template, icl_keys, iia_keys, x_iia_keys)
    'americasnli-spa': (
        'Predice la etiqueta de implicación del siguiente par de oraciones:\n{context}\n{query}',
        'Premisa: "{premise}"; Hipótesis: "{hypothesis}" => {label}',
        'En {language}, "{premise_1}" significa "{premise_2}" y "{hypothesis_1}" significa "{hypothesis_2}"',
        ['premise', 'hypothesis'], ['premise_1', 'hypothesis_1'], ['premise_2', 'hypothesis_2']
    ),
    'americasnli': (
        'Predict the entailment label of the following pair of sentences:\n{context}\n{query}',
        'Premise: "{premise}"; Hypothesis: "{hypothesis}" => {label}',
        'In {language}, "{premise_1}" means "{premise_2}" and "{hypothesis_1}" means "{hypothesis_2}"',
        ['premise', 'hypothesis'], ['premise_1', 'hypothesis_1'], ['premise_2', 'hypothesis_2']
    ),
    'nusatranslation-ind': (
        'Prediksikan label sentimen dari kalimat berikut:\n{context}\n{query}',
        '{text} => {label}',
        'Dalam bahasa {language}, "{text_1}" artinya "{text_2}"',
        'text', 'text_1', 'text_2'
    ),
    'nusatranslation': (
        'Predict the sentiment label of the following sentence:\n{context}\n{query}',
        '{text} => {label}',
        'In {language}, "{text_1}" means "{text_2}"',
        'text', 'text_1', 'text_2'
    ),
    'masakhanews': (
        'Predict the topic of the following news title:\n{context}\n{query}',
        '{text} => {label}',
        'In {language}, "{text_1}" means "{text_2}"',
        'text', 'text_1', 'text_2'
    ),
    'tweetsentimulti': (
        'Predict the sentiment label of the following tweet:\n{context}\n{query}',
        '{text} => {label}',
        'In {language}, "{text_1}" means "{text_2}"',
        'text', 'text_1', 'text_2'
    ),
}

if __name__ == '__main__':
    if len(sys.argv) < 2:
        raise ValueError('main_icl_alignment_translate.py <model_path_or_name> <dataset_name> <icl_type> <icl_index_type> <icl_num_exemplar> <sbert_type> <mt_type> <batch_size>')

    BASE_PATH='./dataset'
    MODEL = sys.argv[1]
    DATASET_NAME = sys.argv[2] # americasnli, nusatranslation, masakhanews
    ICL_TYPE = sys.argv[3] # none, translate
    ICL_INDEX_TYPE = sys.argv[4].split(',') # random, unique, count, tf-idf, sbert
    ICL_EXEMPLAR_COUNT = int(sys.argv[5])
    SBERT_TYPE = sys.argv[6] # type of the SBERT model
    MT_TYPE = sys.argv[7] # type of the MT model
    BATCH_SIZE= int(sys.argv[8])
    
    sbert_id = SBERT_TYPE.split("/")[-1]
    mt_id = MT_TYPE.split("/")[-1]
    SAVE_NAME = f'icl-{ICL_TYPE}-{"$".join(ICL_INDEX_TYPE)}-{ICL_EXEMPLAR_COUNT}_iia-none-none-0_ioa-none_align-none_{sbert_id}_{mt_id}'
    os.makedirs('./metrics_icl_translate', exist_ok=True) 
    os.makedirs('./outputs_icl_translate', exist_ok=True) 

    # Load Dataset
    print('Load Datasets...')
    eval_dsets, icl_dsets, xicl_lang, iia_dsets, itc_dsets, ioa_df = load_dataset(dataset=DATASET_NAME, base_path=BASE_PATH)

    print(f'Loaded {len(eval_dsets)} datasets')
    for i, dset_subset in enumerate(eval_dsets.keys()):
        print(f'{i} {dset_subset}')
    
    # Load Model
    tokenizer = AutoTokenizer.from_pretrained(MODEL, truncation_side='left')
    if "bloom" in MODEL or "xglm" in MODEL or "gpt2" in MODEL:
        model = AutoModelForCausalLM.from_pretrained(MODEL, device_map="auto", load_in_8bit=True)
    else:
        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL, device_map="auto", load_in_8bit=True)
        tokenizer.pad_token = tokenizer.eos_token # Use EOS to pad label
    # model = torch.compile(model)    
    model.eval()
    
    # Load MT Model
    mt_model = AutoModelForSeq2SeqLM.from_pretrained(MT_TYPE).to('cuda:0')
    mt_model.eval()

    metrics = {
        'dataset': [], 'lang': [],
        'accuracy': [], 'macro_f1': [], 'weighted_f1': []
    }
    
    for dset_lang, eval_dset in eval_dsets.items():
        if dset_lang == 'eng':
            continue
        if bcp47_lang_map[dset_lang] is None:
            continue

        print(f'Processing {DATASET_NAME} {dset_lang}')

        ###
        # Preprocessing
        ###

        # Extract Metadata
        prompt_template, icl_template, iia_template, icl_keys, iia_keys, x_iia_keys = dataset_to_metadata_map[DATASET_NAME]
        
        # Prepare Prompter
        icl_prompter = ICLPrompter(
            prompt_template=prompt_template, icl_template=icl_template, iia_template=iia_template
        )

        # Retrieve & preprocess labels
        label_names = list(set(eval_dset['label']))
        
        ###
        # Indexing
        ###
        if ICL_TYPE == 'translate':
            icl_dset = icl_dsets[xicl_lang]
            icl_indexer = SimpleDatasetIndexer(dataset=icl_dset, index_key=icl_keys, index_type=ICL_INDEX_TYPE, sbert_type=SBERT_TYPE)
        else:
            icl_dset = None
            icl_indexer = None
        
        ###
        # Machine Translation
        ###        
        mt_tokenizer = AutoTokenizer.from_pretrained(MT_TYPE, src_lang=bcp47_lang_map[dset_lang])

        ###
        # Inference
        ###            
        inputs, preds, golds = [], [], []

        # Check saved data
        if exists(f'outputs_icl_translate/icl-alignment_{DATASET_NAME}_{dset_lang}_{MODEL.split("/")[-1]}_{SAVE_NAME}.csv'):
            print("Output exist, use partial log instead")
            with open(f'outputs_icl_translate/icl-alignment_{DATASET_NAME}_{dset_lang}_{MODEL.split("/")[-1]}_{SAVE_NAME}.csv') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    inputs.append(row["Input"])
                    preds.append(row["Pred"])
                    golds.append(row["Gold"])
            print(f"Skipping until {len(preds)}")

        # Perform Inference
        prompts, labels = [], []
        if len(preds) < len(eval_dset):
            for e, sample in enumerate(tqdm(eval_dset)):
                if e < len(preds):
                    continue

                ###
                # Translate Samples
                ###
                
                if type(icl_keys) == str:
                    input_keys = [icl_keys]
                else:
                    input_keys = icl_keys
                
                for key in input_keys:
                    text = sample[key]
                    mt_inputs = mt_tokenizer(text, padding='longest', return_tensors="pt").to('cuda:0')
                    translated_tokens = mt_model.generate(
                        **mt_inputs, forced_bos_token_id=mt_tokenizer.lang_code_to_id[bcp47_lang_map[xicl_lang]], max_length=100
                    )
                    sample[key] = mt_tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
                
                ###
                # Extract Features
                ###
                if type(icl_keys) == str:
                    input_query = sample[icl_keys]
                else: # type(icl_keys) == list
                    input_query = [sample[key] for key in icl_keys]
                label = sample['label']
                
                ###
                # Retrieve Exemplars
                ###
                
                # Retrieve ICL Exemplars
                if icl_indexer is not None:
                    icl_samples = icl_indexer.get_similar_samples(input_query, n_samples=ICL_EXEMPLAR_COUNT)
                else:
                    icl_samples = None

                ###
                # Prepare Zero-Shot / Few-Shot Prompt Text
                ###
                prompt_text = icl_prompter.generate_prompt(
                    input_exemplar=sample,
                    icl_exemplars=icl_samples,
                    input_alignment_exemplars=None,
                    output_alignment_prompt=None,
                    alignment_language=lang_map[dset_lang]
                )
                
                prompts.append(prompt_text)
                labels.append(label)

                ###
                # Perform zero-shot / few-shot Inference
                ###
                
                # Batch Inference
                if len(prompts) == BATCH_SIZE:
                    out = predict_classification_batch(model, tokenizer, prompts, label_names, device='cuda:0')
                    hyps = torch.argmax(torch.stack(out, dim=-1), dim=-1).tolist()
                    for (prompt, hyp, label) in zip(prompts, hyps, labels):
                        inputs.append(prompt)
                        preds.append(label_names[int(hyp)])
                        golds.append(label)
                    # print(f'label_names[int(hyp)]: ' + ', '.join([label_names[int(hyp)] for hyp in hyps]))
                    # print(f'labels: ' + ', '.join(labels))
                    prompts, labels = [], []                    

                # partial saving
                if len(preds) % (5 * BATCH_SIZE) == 0:
                    inference_df = pd.DataFrame(list(zip(inputs, preds, golds)), columns =["Input", 'Pred', 'Gold'])
                    inference_df.to_csv(f'outputs_icl_translate/icl-alignment_{DATASET_NAME}_{dset_lang}_{MODEL.split("/")[-1]}_{SAVE_NAME}.csv', index=False)
                   
            # Perform zero-shot / few-shot Inference on last remaining batch data
            if len(prompts) > 0:
                out = predict_classification_batch(model, tokenizer, prompts, label_names, device='cuda:0')
                hyps = torch.argmax(torch.stack(out, dim=-1), dim=-1).tolist()
                for (prompt, hyp, label) in zip(prompts, hyps, labels):
                    inputs.append(prompt)
                    preds.append(label_names[int(hyp)])
                    golds.append(label)
                # print(f'label_names[int(hyp)]: ' + ', '.join([label_names[int(hyp)] for hyp in hyps]))
                # print(f'labels: ' + ', '.join(labels))
                prompts, labels = [], []
                
        # Full saving
        inference_df = pd.DataFrame(list(zip(inputs, preds, golds)), columns =["Input", 'Pred', 'Gold'])
        inference_df.to_csv(f'outputs_icl_translate/icl-alignment_{DATASET_NAME}_{dset_lang}_{MODEL.split("/")[-1]}_{SAVE_NAME}.csv', index=False)

        cls_report = classification_report(golds, preds, output_dict=True)
        acc, macro_f1, weighted_f1 = cls_report['accuracy'], cls_report['macro avg']['f1-score'], cls_report['weighted avg']['f1-score']
        print(f'{DATASET_NAME} {dset_lang}')
        print('accuracy', acc)
        print('f1 macro', macro_f1)
        print('f1 weighted', weighted_f1)
        print("===\n\n")

        metrics['dataset'].append(DATASET_NAME)
        metrics['lang'].append(dset_lang)
        metrics['accuracy'].append(acc)
        metrics['macro_f1'].append(macro_f1)
        metrics['weighted_f1'].append(weighted_f1)

    pd.DataFrame.from_dict(metrics).T.reset_index().to_csv(f'metrics_icl_translate/results_{DATASET_NAME}_{MODEL.split("/")[-1]}_{SAVE_NAME}.csv', index=False)