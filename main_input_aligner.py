"""nusacrowd zero-shot prompt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ru8DyS2ALWfRdkjOPHj-KNjw6Pfa44Nd
"""
import os, sys
import csv
from os.path import exists
import glob
import random

import numpy as np
from numpy import argmax
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import f1_score, accuracy_score, classification_report
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

import torch
import torch.nn.functional as F
import datasets
from sentence_transformers import SentenceTransformer

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM

DEBUG=False

lang_map = {
    'btk': 'Batak', 'bew': 'Betawi', 'sun': 'Sundanese', 'jav': 'Javanese',
    'mad': 'Madurese', 'mak': 'Makassarese', 'min': 'Minangkabau', 'mui': 'Musi', 
    'rej': 'Rejang', 'abs': 'Ambonese', 'bhp': 'Bima'
}

#  Prompting & Predict
def to_prompt(input, prompt, labels, parallel_texts=None, parallel_langs=None):
    # single label
    if 'text' in input:
        prompt = prompt.replace('[INPUT]', input['text'])
    else:
        prompt = prompt.replace('[INPUT_A]', input['text_1'])
        prompt = prompt.replace('[INPUT_B]', input['text_2'])

    # replace [OPTIONS] to A, B, or C
    if "[OPTIONS]" in prompt:
        new_labels = [f'{l}' for l in labels]
        new_labels[-1] = "or " + new_labels[-1] 
        if len(new_labels) > 2:
            prompt = prompt.replace('[OPTIONS]', ', '.join(new_labels))
        else:
            prompt = prompt.replace('[OPTIONS]', ' '.join(new_labels))

    # Add parallel context
    if parallel_texts is not None:
        if type(parallel_texts) is not list:
            parallel_texts = [parallel_texts]
        
        lang_1, lang_2 = parallel_langs
        prefixes = []
        for text_1, text_2 in parallel_texts:
            prefixes.append(f'{lang_1}: {text_1}\n{lang_2}: {text_2}')
        prefix = '\n\n'.join(prefixes)
        prompt = f'{prefix}\n\nGiven the above {lang_1}-{lang_2} parallel sentences context for understanding {lang_2}; {prompt}'

    return prompt

@torch.no_grad()
def get_logprobs(model, tokenizer, prompt, label_ids=None, label_attn=None):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to('cuda')
    input_ids, output_ids = inputs["input_ids"], inputs["input_ids"][:, 1:]
    
    if model.config.is_encoder_decoder:
        outputs = model(**inputs, labels=label_ids)
        logits = outputs.logits
        
        logprobs = torch.gather(F.log_softmax(logits, dim=2), 2, label_ids.unsqueeze(2)) * label_attn.unsqueeze(2)
        return logprobs.sum(dim=-1)
    else:    
        outputs = model(**inputs, labels=input_ids)
        logits = outputs.logits
        
        logprobs = torch.gather(F.log_softmax(logits, dim=2), 2, output_ids.unsqueeze(2))
        return logprobs[:,-label_ids.shape[-1]:].sum(dim=-1)

def predict_classification(model, tokenizer, prompt, labels):
    labels_encoded = tokenizer(labels, add_special_tokens=False, padding=True, return_tensors='pt')
    list_label_ids = labels_encoded['input_ids'].to('cuda')
    list_label_attn = labels_encoded['attention_mask'].to('cuda')
    if model.config.is_encoder_decoder:
        probs = [
            get_logprobs(model, tokenizer, prompt.replace('[LABELS_CHOICE]', ''), label_ids.view(1,-1), label_attn.view(1,-1)) 
            for (label_ids, label_attn) in zip(list_label_ids, list_label_attn)
        ]
    else:
        probs = [
            get_logprobs(model, tokenizer, prompt.replace('[LABELS_CHOICE]', label), label_ids.view(1,-1), label_attn.view(1,-1)) 
            for (label, label_ids, label_attn) in zip(labels, list_label_ids, list_label_attn)
        ]
    return probs

if __name__ == '__main__':
    if len(sys.argv) < 2:
        raise ValueError('main_input_aligner.py <model_path_or_name> <[OPTIONAL] exemplar_type> <num_exemplar>')

    MODEL = sys.argv[1]
    EXEMPLAR_TYPE = sys.argv[2]
    EXEMPLAR_COUNT = int(sys.argv[3])
    EXEMPLAR_NAME = f'{EXEMPLAR_TYPE}-{EXEMPLAR_COUNT}'

    os.makedirs('./metrics_aligner', exist_ok=True) 
    os.makedirs('./outputs_aligner', exist_ok=True) 

    # Load Prompt
    prompt_templates = get_prompt()

    # Load Dataset
    print('Load NLU Datasets...')
    nlu_dsets, nlg_dsets = load_nlu_tasks(), load_nlg_tasks()

    print(f'Loaded {len(nlu_dsets)} NLU datasets')
    for i, dset_subset in enumerate(nlu_dsets.keys()):
        print(f'{i} {dset_subset}')
        
    print(f'Loaded {len(nlg_dsets)} NLG datasets')
    for i, dset_subset in enumerate(nlg_dsets.keys()):
        print(f'{i} {dset_subset}')

    # Load Model
    tokenizer = AutoTokenizer.from_pretrained(MODEL, truncation_side='left')
    if "bloom" in MODEL or "xglm" in MODEL or "gpt2" in MODEL:
        model = AutoModelForCausalLM.from_pretrained(MODEL, device_map="auto", load_in_8bit=True)
    else:
        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL, device_map="auto", load_in_8bit=True)
        tokenizer.pad_token = tokenizer.eos_token # Use EOS to pad label
        
    model.eval()
    torch.no_grad()

    metrics = {
        'dataset': [], 'task': [], 'lang': [], 'prompt_id': [],
        'accuracy': [], 'macro_f1': [], 'weighted_f1': []
    }
    
    for key, nlu_dset in nlu_dsets.items():
        # Retrieve metadata
        dset_name, task, lang = key
        test_dset = nlu_dset['test']        
        print(f'Processing {dset_name}-{task}-{lang}')

        # Retrieve & preprocess labels
        label_names = list(set(test_dset['label']))
        label_names = [str(label).lower().replace("_"," ") for label in label_names]
        label_to_id_dict = { l : i for i, l in enumerate(label_names)}
            
        for prompt_id, prompt_template in enumerate(prompt_templates[task]):
            if prompt_id not in [0, 1, 3]:
                continue

            inputs, preds, golds = [], [], []
            
            # Check saved data
            if exists(f'outputs_aligner/input-align_{dset_name}_{task}_{lang}_{prompt_id}_{MODEL.split("/")[-1]}_{EXEMPLAR_NAME}.csv'):
                print("Output exist, use partial log instead")
                with open(f'outputs_aligner/input-align-{dset_name}_{task}_{lang}_{prompt_id}_{MODEL.split("/")[-1]}_{EXEMPLAR_NAME}.csv') as csvfile:
                    reader = csv.DictReader(csvfile)
                    for row in reader:
                        inputs.append(row["Input"])
                        preds.append(row["Pred"])
                        golds.append(row["Gold"])
                print(f"Skipping until {len(preds)}")
            
            # Prepare parallel data
            nlg_dset = nlg_dsets[(dset_name, 'mt', lang)]
            nlg_df = pd.concat([
                nlg_dset['train'].to_pandas(), nlg_dset['valid'].to_pandas(), nlg_dset['test'].to_pandas()
            ]).set_index('id')
            
            ###
            # Prepare Exemplars Indexer
            ###
            if 'mt-' in EXEMPLAR_TYPE:
                if '-eng' in EXEMPLAR_TYPE:
                    mt_path = f'./mt/nusa_kalimat-emot-{lang}-eng.csv'
                else:
                    mt_path = f'./mt/nusa_kalimat-emot-{lang}.csv'
                    
                if not exists(mt_path):
                    # Skip non-existance language
                    continue
                    
                # Overwrite NLG Dataframe
                nlg_df = pd.read_csv(mt_path)
                nlg_df = nlg_df.rename(columns={"text": "tgt_text", "mt_text": "src_text"}).set_index('id')
            elif EXEMPLAR_TYPE == ['similar', 'similar_bible']:
                # Prepare Index
                text_corpus = nlg_df['tgt_text'].values

                tfidfer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3), min_df=3)
                counter = CountVectorizer(max_features=10000, ngram_range=(1, 3), min_df=3, binary=True)

                tfidf_vec = tfidfer.fit_transform(text_corpus)
                count_vec = counter.fit_transform(text_corpus)
                count_sum = np.array(count_vec.sum(axis=-1)).squeeze()

            elif EXEMPLAR_TYPE == 'similar-sbert-cross':
                # Load Model & Encode Sentences
                src_sents, tgt_sents = nlg_df['src_text'].values, nlg_df['tgt_text'].values
                if not os.path.exists('cache/sbert-nusawrites-cross.pt'):
                    sbert = SentenceTransformer('sentence-transformers/stsb-xlm-r-multilingual')
                    embeddings = sbert.encode(src_sents, batch_size=128, device='cuda:0', show_progress_bar=True, convert_to_tensor=True)
                    torch.save(embeddings.cpu(), f'cache/similar-sbert-cross-{dset_name}_{lang}.pt')
                else:
                    embeddings = torch.load(f'cache/similar-sbert-cross-{dset_name}_{lang}.pt').cuda()
                
            elif EXEMPLAR_TYPE == 'similar-sbert-mono':
                # Load Model
                src_sents, tgt_sents = nlg_df['src_text'].values, nlg_df['tgt_text'].values
                if not os.path.exists('cache/sbert-nusawrites-mono.pt'):
                    sentences = ["This is an example sentence", "Each sentence is converted"]
                    sbert = SentenceTransformer('sentence-transformers/stsb-xlm-r-multilingual')
                    embeddings = sbert.encode(tgt_sents, batch_size=128, device='cuda:0', show_progress_bar=True, convert_to_tensor=True)
                    torch.save(embeddings.cpu(), f'cache/similar-sbert-mono-{dset_name}_{lang}.pt')
                else:
                    embeddings = torch.load(f'cache/similar-sbert-mono-{dset_name}_{lang}.pt').cuda()
                    
            ###
            # Run Inference
            ###
            if len(preds) < len(test_dset):
                for e, sample in enumerate(tqdm(test_dset)):
                    if e < len(preds):
                        continue

                    ###
                    # Prepare Exemplars
                    ###
                    if 'random' in EXEMPLAR_TYPE:
                        parallel_texts = []
                        for i in range(EXEMPLAR_COUNT):
                            idx = random.randint(0, len(nlg_df)-1)
                            parallel_texts.append((nlg_df.iloc[idx]['src_text'], nlg_df.iloc[idx]['tgt_text']))
                    elif 'EXEMPLAR_TYPE' in ['similar', 'similar-bible']:
                        num_sort = EXEMPLAR_COUNT
                        z1 = tfidfer.transform([sample['text']])
                        z2 = counter.transform([sample['text']])

                        tfidf_score = np.squeeze(tfidf_vec.dot(z1.T).toarray())
                        count_score = (np.squeeze(count_vec.dot(z2.T).toarray()) / (count_sum + np.finfo(np.float64).eps))
                        sim_score = (tfidf_score + count_score) / 2
                        sim_idx = np.argpartition(sim_score, kth=-num_sort, axis=0)[-num_sort:]

                        parallel_texts = []
                        for src_text, tgt_text in zip(sim_samples['src_text'], sim_samples['tgt_text']):
                            parallel_texts.append((src_text, tgt_text))
                    elif 'similar-sbert' in EXEMPLAR_TYPE:
                        sent_embed = sbert.encode([sample['text']], device='cuda:0', convert_to_tensor=True)
                        parallel_texts = []
                        for idx in torch.topk(sent_embed @ (embeddings.T), EXEMPLAR_COUNT).indices:
                            parallel_texts.append((src_text, tgt_text))
                    else: # EXEMPLAR_TYPE = none
                        parallel_texts = None

                    ###
                    # Prepare Zero-Shot / Few-Shot Prompt Text
                    ###                        
                    input_text, label = sample['text'], sample['label']
                    if parallel_texts is not None: 
                        # EXEMPLAR_TYPE in [translation, random, similar]
                        # Add translation prefix
                        if '-eng' in EXEMPLAR_TYPE:
                            prompt_text = to_prompt(sample, prompt_template, label_names, parallel_texts, ('English', lang_map[lang]))
                        else:
                            prompt_text = to_prompt(sample, prompt_template, label_names, parallel_texts, ('Indonesian', lang_map[lang]))
                    else:
                        # EXEMPLAR_TYPE = none
                        # Don't use any prefix augmentation
                        prompt_text = to_prompt(sample, prompt_template, label_names, None, None)
                    
                    ###
                    # Perform zero-shot / few-shot Inference
                    ###
                    with torch.inference_mode():
                        out = predict_classification(model, tokenizer, prompt_text, label_names)
                        pred = argmax([o.cpu().detach() for o in out])

                    inputs.append(prompt_text)
                    preds.append(label_names[int(pred)])
                    golds.append(sample['label'])

                    # partial saving
                    if len(preds) % 100 == 0:
                        inference_df = pd.DataFrame(list(zip(inputs, preds, golds)), columns =["Input", 'Pred', 'Gold'])
                        inference_df.to_csv(f'outputs_aligner/input-align-{dset_name}_{task}_{lang}_{prompt_id}_{MODEL.split("/")[-1]}_{EXEMPLAR_NAME}.csv', index=False)
                # full saving
                inference_df = pd.DataFrame(list(zip(inputs, preds, golds)), columns =["Input", 'Pred', 'Gold'])
                inference_df.to_csv(f'outputs_aligner/input-align-{dset_name}_{task}_{lang}_{prompt_id}_{MODEL.split("/")[-1]}_{EXEMPLAR_NAME}.csv', index=False)

            cls_report = classification_report(golds, preds, output_dict=True)
            acc, macro_f1, weighted_f1 = cls_report['accuracy'], cls_report['macro avg']['f1-score'], cls_report['weighted avg']['f1-score']
            print(f'{dset_name}_{task}_{lang}')
            print('accuracy', acc)
            print('f1 macro', macro_f1)
            print('f1 weighted', weighted_f1)
            print("===\n\n")
            
            metrics['dataset'].append(dset_name)
            metrics['task'].append(task)
            metrics['lang'].append(lang)
            metrics['prompt_id'].append(prompt_id)
            metrics['accuracy'].append(acc)
            metrics['macro_f1'].append(macro_f1)
            metrics['weighted_f1'].append(weighted_f1)

    pd.DataFrame.from_dict(metrics).T.reset_index().to_csv(f'metrics_aligners/results_{MODEL.split("/")[-1]}_{EXEMPLAR_NAME}.csv', index=False)