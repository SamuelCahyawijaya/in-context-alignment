{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f39529-44af-440d-b721-a06d1424fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7fe3d9-884c-4465-a513-c25631bb119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download XNLI Data\n",
    "!wget https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip\n",
    "!unzip XNLI-1.0.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538b05f-6644-4ef2-9b03-127cc5d5e65e",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738ba5a-624f-4428-8f13-992fcf9d977f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NusaTranslation Senti\n",
    "nt_senti_dset = {\n",
    "\t\"btk\": load_dataset(\"indonlp/nusatranslation_senti\", name=\"nusatranslation_senti_btk_nusantara_text\"),\n",
    "\t\"sun\": load_dataset(\"indonlp/nusatranslation_senti\", name=\"nusatranslation_senti_sun_nusantara_text\"),\n",
    "\t\"jav\": load_dataset(\"indonlp/nusatranslation_senti\", name=\"nusatranslation_senti_jav_nusantara_text\"),\n",
    "\t\"mad\": load_dataset(\"indonlp/nusatranslation_senti\", name=\"nusatranslation_senti_mad_nusantara_text\"),\n",
    "\t\"mak\": load_dataset(\"indonlp/nusatranslation_senti\", name=\"nusatranslation_senti_mak_nusantara_text\"),\n",
    "\t\"min\": load_dataset(\"indonlp/nusatranslation_senti\", name=\"nusatranslation_senti_min_nusantara_text\"),\n",
    "}\n",
    "\n",
    "# NusaTranslation MT\n",
    "nt_mt_dset = {\n",
    "\t\"btk\": load_dataset(\"indonlp/nusatranslation_mt\", name=\"nusatranslation_mt_btk_ind_nusantara_t2t\"),\n",
    "\t\"sun\": load_dataset(\"indonlp/nusatranslation_mt\", name=\"nusatranslation_mt_sun_ind_nusantara_t2t\"),\n",
    "\t\"jav\": load_dataset(\"indonlp/nusatranslation_mt\", name=\"nusatranslation_mt_jav_ind_nusantara_t2t\"),\n",
    "\t\"mad\": load_dataset(\"indonlp/nusatranslation_mt\", name=\"nusatranslation_mt_mad_ind_nusantara_t2t\"),\n",
    "\t\"mak\": load_dataset(\"indonlp/nusatranslation_mt\", name=\"nusatranslation_mt_mak_ind_nusantara_t2t\"),\n",
    "\t\"min\": load_dataset(\"indonlp/nusatranslation_mt\", name=\"nusatranslation_mt_min_ind_nusantara_t2t\"),\n",
    "}\n",
    "\n",
    "# NusaX Senti\n",
    "nusax_senti_dset = {\n",
    "\t\"btk\": load_dataset(\"indonlp/NusaX-senti\", name=\"bbc\"),\n",
    "\t\"sun\": load_dataset(\"indonlp/NusaX-senti\", name=\"sun\"),\n",
    "\t\"jav\": load_dataset(\"indonlp/NusaX-senti\", name=\"jav\"),\n",
    "\t\"mad\": load_dataset(\"indonlp/NusaX-senti\", name=\"mad\"),\n",
    "\t\"mak\": load_dataset(\"indonlp/NusaX-senti\", name=\"bug\"),\n",
    "\t\"min\": load_dataset(\"indonlp/NusaX-senti\", name=\"min\"),\n",
    "\t\"ind\": load_dataset(\"indonlp/NusaX-senti\", name=\"ind\"), # For X-ICL\n",
    "\t\"eng\": load_dataset(\"indonlp/NusaX-senti\", name=\"eng\"), # For X-ICL\n",
    "}\n",
    "\n",
    "# NusaX MT ind\n",
    "nusax_mt_ind_dset = {\n",
    "\t\"btk\": load_dataset(\"indonlp/NusaX-MT\", name=\"bbc-ind\"), # For TIA\n",
    "\t\"sun\": load_dataset(\"indonlp/NusaX-MT\", name=\"sun-ind\"), # For TIA\n",
    "\t\"jav\": load_dataset(\"indonlp/NusaX-MT\", name=\"jav-ind\"), # For TIA\n",
    "\t\"mad\": load_dataset(\"indonlp/NusaX-MT\", name=\"mad-ind\"), # For TIA\n",
    "\t\"mak\": load_dataset(\"indonlp/NusaX-MT\", name=\"bug-ind\"), # For TIA\n",
    "\t\"min\": load_dataset(\"indonlp/NusaX-MT\", name=\"min-ind\"), # For TIA\n",
    "}\n",
    "\n",
    "# NusaX MT eng (Extended experiment)\n",
    "nusax_mt_eng_dset = {\n",
    "\t\"btk\": load_dataset(\"indonlp/NusaX-MT\", name=\"bbc-eng\"), # For TIA\n",
    "\t\"sun\": load_dataset(\"indonlp/NusaX-MT\", name=\"sun-eng\"), # For TIA\n",
    "\t\"jav\": load_dataset(\"indonlp/NusaX-MT\", name=\"jav-eng\"), # For TIA\n",
    "\t\"mad\": load_dataset(\"indonlp/NusaX-MT\", name=\"mad-eng\"), # For TIA\n",
    "\t\"mak\": load_dataset(\"indonlp/NusaX-MT\", name=\"bug-eng\"), # For TIA\n",
    "\t\"min\": load_dataset(\"indonlp/NusaX-MT\", name=\"min-eng\"), # For TIA\n",
    "}\n",
    "\n",
    "# MasakhaNews\n",
    "masakhanews_dset = {\n",
    "\t\"amh\": load_dataset(\"masakhane/masakhanews\", name=\"amh\"),\n",
    "\t\"hau\": load_dataset(\"masakhane/masakhanews\", name=\"hau\"),\n",
    "\t\"ibo\": load_dataset(\"masakhane/masakhanews\", name=\"ibo\"),\n",
    "\t\"lug\": load_dataset(\"masakhane/masakhanews\", name=\"lug\"),\n",
    "\t\"pcm\": load_dataset(\"masakhane/masakhanews\", name=\"pcm\"),\n",
    "\t\"sna\": load_dataset(\"masakhane/masakhanews\", name=\"sna\"),\n",
    "\t\"swa\": load_dataset(\"masakhane/masakhanews\", name=\"swa\"),\n",
    "\t\"xho\": load_dataset(\"masakhane/masakhanews\", name=\"xho\"),\n",
    "\t\"yor\": load_dataset(\"masakhane/masakhanews\", name=\"yor\"),\n",
    "\t\"eng\": load_dataset(\"masakhane/masakhanews\", name=\"eng\"), # For X-ICL\n",
    "}\n",
    "\n",
    "# MAFAND\n",
    "mafand_dset = {\n",
    "\t\"amh\": load_dataset(\"masakhane/mafand\", name=\"en-amh\"), # For TIA\n",
    "\t\"hau\": load_dataset(\"masakhane/mafand\", name=\"en-hau\"), # For TIA\n",
    "\t\"ibo\": load_dataset(\"masakhane/mafand\", name=\"en-ibo\"), # For TIA\n",
    "\t\"lug\": load_dataset(\"masakhane/mafand\", name=\"en-lug\"), # For TIA\n",
    "\t\"pcm\": load_dataset(\"masakhane/mafand\", name=\"en-pcm\"), # For TIA\n",
    "\t\"sna\": load_dataset(\"masakhane/mafand\", name=\"en-sna\"), # For TIA\n",
    "\t\"swa\": load_dataset(\"masakhane/mafand\", name=\"en-swa\"), # For TIA\n",
    "\t\"xho\": load_dataset(\"masakhane/mafand\", name=\"en-xho\"), # For TIA\n",
    "\t\"yor\": load_dataset(\"masakhane/mafand\", name=\"en-yor\"), # For TIA\n",
    "}\n",
    "\n",
    "# AmericasNLI\n",
    "americasnli_dset = {\n",
    "\t\"aym\": load_dataset(\"americas_nli\", name=\"aym\"), # For TIA\n",
    "\t\"bzd\": load_dataset(\"americas_nli\", name=\"bzd\"), # For TIA\n",
    "\t\"cni\": load_dataset(\"americas_nli\", name=\"cni\"), # For TIA\n",
    "\t\"gn\": load_dataset(\"americas_nli\", name=\"gn\"), # For TIA\n",
    "\t\"hch\": load_dataset(\"americas_nli\", name=\"hch\"), # For TIA\n",
    "\t\"nah\": load_dataset(\"americas_nli\", name=\"nah\"), # For TIA\n",
    "\t\"oto\": load_dataset(\"americas_nli\", name=\"oto\"), # For TIA\n",
    "\t\"quy\": load_dataset(\"americas_nli\", name=\"quy\"), # For TIA\n",
    "\t\"shp\": load_dataset(\"americas_nli\", name=\"shp\"), # For TIA\n",
    "\t\"tar\": load_dataset(\"americas_nli\", name=\"tar\"), # For TIA\n",
    "}\n",
    "\n",
    "# XNLI\n",
    "xnli_dset = load_dataset(\"xnli\", name=\"es\") # For X-ICL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046006c2-fc6d-4667-b731-dc2b6a1ed104",
   "metadata": {},
   "source": [
    "# Standardize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e76ea2-4a43-44a7-8588-61463aa32a3d",
   "metadata": {},
   "source": [
    "### NLU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f32ee-e1e8-4878-bb67-6b80d440b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '.'\n",
    "\n",
    "def label2str(row, dset):\n",
    "    row['str_label'] = dset.features['label'].int2str(row['label'])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4369c-91ca-4ea7-8da7-7781585286c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Prepare Evaluation Data\n",
    "#\n",
    "# Single-Sentence Classification [text, label]\n",
    "# NLI Classification [premise, hypothesis, label]\n",
    "###\n",
    "\n",
    "# Process NusaTranslation Senti\n",
    "nt_senti_dset_clean = {}\n",
    "for key in nt_senti_dset.keys():\n",
    "    dset = nt_senti_dset[key]['test'].remove_columns(['id'])\n",
    "    nt_senti_dset_clean[key] = dset\n",
    "nt_senti_dset_clean = datasets.DatasetDict(nt_senti_dset_clean)\n",
    "nt_senti_dset_clean = nt_senti_dset_clean.map(\n",
    "    label2str, remove_columns=['label'], fn_kwargs={\"dset\": nusax_senti_dset['jav']['train']}\n",
    ").rename_columns({'str_label': 'label'})\n",
    "\n",
    "# Process MasakhaNews Senti\n",
    "masakhanews_dset_clean = {}\n",
    "for key in masakhanews_dset.keys():\n",
    "    dset = masakhanews_dset[key]['test'].remove_columns(['text', 'headline_text', 'url'])\n",
    "    dset = dset.rename_columns({'headline': 'text'})\n",
    "    dset = dset.map(\n",
    "        label2str, remove_columns=['label'], fn_kwargs={\"dset\": dset}\n",
    "    ).rename_columns({'str_label': 'label'})\n",
    "    masakhanews_dset_clean[key] = dset\n",
    "masakhanews_dset_clean = datasets.DatasetDict(masakhanews_dset_clean)\n",
    "\n",
    "# Process AmericasNLI Senti\n",
    "americasnli_dset_clean = {}\n",
    "for key in americasnli_dset.keys():\n",
    "    dset = americasnli_dset[key]['test']\n",
    "    americasnli_dset_clean[key] = dset\n",
    "americasnli_dset_clean = datasets.DatasetDict(americasnli_dset_clean)\n",
    "americasnli_dset_clean = americasnli_dset_clean.map(\n",
    "    label2str, remove_columns=['label'], fn_kwargs={\"dset\": americasnli_dset['aym']['test']}\n",
    ").rename_columns({'str_label': 'label'})\n",
    "\n",
    "# Save to Cache\n",
    "nt_senti_dset_clean.save_to_disk(f'{save_path}/nt_senti_test_dset')\n",
    "masakhanews_dset_clean.save_to_disk(f'{save_path}/masakhanews_test_dset')\n",
    "americasnli_dset_clean.save_to_disk(f'{save_path}/americasnli_test_dset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428c145-b47a-43e6-94de-41717d5a9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Prepare ICL & X-ICL Data\n",
    "#\n",
    "# Single-Sentence Classification [text, label]\n",
    "# NLI Classification [premise, hypothesis, label]\n",
    "###\n",
    "\n",
    "# Process NusaTranslation Senti ICL Data => NusaX Senti All Splits\n",
    "icl_nusax_senti_dset_clean = {}\n",
    "for key in nusax_senti_dset.keys():\n",
    "    dset = []\n",
    "    for split in nusax_senti_dset[key].keys():\n",
    "        dset.append(nusax_senti_dset[key][split].remove_columns(['id', 'lang']))\n",
    "    icl_nusax_senti_dset_clean[key] = datasets.concatenate_datasets(dset)\n",
    "icl_nusax_senti_dset_clean = datasets.DatasetDict(icl_nusax_senti_dset_clean)\n",
    "icl_nusax_senti_dset_clean = icl_nusax_senti_dset_clean.map(\n",
    "    label2str, remove_columns=['label'], fn_kwargs={\"dset\": nusax_senti_dset['jav']['train']}\n",
    ").rename_columns({'str_label': 'label'})\n",
    "\n",
    "# Process MasakhaNews ICL Data => MasakhaNews Train & Validation\n",
    "icl_masakhanews_dset_clean = {}\n",
    "for key in masakhanews_dset.keys():\n",
    "    dset = []\n",
    "    for split in masakhanews_dset[key].keys():\n",
    "        dset.append(masakhanews_dset[key][split].remove_columns(['text', 'headline_text', 'url']))\n",
    "    dset = datasets.concatenate_datasets(dset).rename_columns({'headline': 'text'})\n",
    "    dset = dset.map(\n",
    "        label2str, remove_columns=['label'], fn_kwargs={\"dset\": dset}\n",
    "    ).rename_columns({'str_label': 'label'})\n",
    "    icl_masakhanews_dset_clean[key] = dset\n",
    "icl_masakhanews_dset_clean = datasets.DatasetDict(icl_masakhanews_dset_clean)\n",
    "\n",
    "# Process AmericasNLI ICL Data => ICL: AmericasNLI Validation, X-ICL: XNLI Validation\n",
    "icl_americasnli_dset_clean = {}\n",
    "for key in americasnli_dset.keys():\n",
    "    dset = americasnli_dset[key]['validation']\n",
    "    icl_americasnli_dset_clean[key] = dset\n",
    "icl_americasnli_dset_clean['spa'] = xnli_dset['validation'] # Add Spanish data from XNLI\n",
    "icl_americasnli_dset_clean = datasets.DatasetDict(icl_americasnli_dset_clean)\n",
    "\n",
    "icl_americasnli_dset_clean = icl_americasnli_dset_clean.map(\n",
    "    label2str, remove_columns=['label'], fn_kwargs={\"dset\": americasnli_dset['aym']['test']}\n",
    ").rename_columns({'str_label': 'label'})\n",
    "\n",
    "# Save to Cache\n",
    "icl_nusax_senti_dset_clean.save_to_disk(f'{save_path}/icl_nusax_senti_dset')\n",
    "icl_masakhanews_dset_clean.save_to_disk(f'{save_path}/icl_masakhanews_dset')\n",
    "icl_americasnli_dset_clean.save_to_disk(f'{save_path}/icl_americasnli_dset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29044d0b-cd2e-4e7d-95f8-cab94b36c64c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TIA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72498d8a-7e67-40d6-acfd-05ba36e4f7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d38f5fb6-5b14-4425-b8c9-d59a81c739d4",
   "metadata": {},
   "source": [
    "### ITC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4820e-c5d8-4ffd-a221-19148a138438",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '.'\n",
    "\n",
    "def label2str(row, dset):\n",
    "    row['str_label'] = dset.features['label'].int2str(row['label'])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5688237-3762-4674-b277-4f71c1df6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# NusaX Combined [text_1, text_2, label]\n",
    "####\n",
    "\n",
    "nusax_mt_ind_dset_clean = datasets.load_from_disk(f'{save_path}/nusax_mt_ind_dset')\n",
    "nusax_mt_eng_dset_clean = datasets.load_from_disk(f'{save_path}/nusax_mt_eng_dset')\n",
    "\n",
    "# Process NusaX Combined ind\n",
    "nusax_combined_ind_dset = {}\n",
    "for key in nusax_senti_dset.keys():\n",
    "    if key in ['ind', 'eng']:\n",
    "        continue\n",
    "        \n",
    "    dset = []\n",
    "    for split in nusax_senti_dset[key].keys():\n",
    "        dset.append(nusax_senti_dset[key][split].remove_columns(['id', 'lang', 'text']))\n",
    "    nusax_combined_ind_dset[key] = datasets.concatenate_datasets([\n",
    "        datasets.concatenate_datasets(dset),\n",
    "        nusax_mt_ind_dset_clean[key]\n",
    "    ], axis=1)\n",
    "nusax_combined_ind_dset = datasets.DatasetDict(nusax_combined_ind_dset)\n",
    "nusax_combined_ind_dset = nusax_combined_ind_dset.map(\n",
    "    label2str, remove_columns=['label'], fn_kwargs={\"dset\": nusax_senti_dset['jav']['train']}\n",
    ").rename_columns({'str_label': 'label'})\n",
    "\n",
    "# Process NusaX Combined ind\n",
    "nusax_combined_eng_dset = {}\n",
    "for key in nusax_senti_dset.keys():\n",
    "    if key in ['ind', 'eng']:\n",
    "        continue\n",
    "        \n",
    "    dset = []\n",
    "    for split in nusax_senti_dset[key].keys():\n",
    "        dset.append(nusax_senti_dset[key][split].remove_columns(['id', 'lang', 'text']))\n",
    "    nusax_combined_eng_dset[key] = datasets.concatenate_datasets([\n",
    "        datasets.concatenate_datasets(dset),\n",
    "        nusax_mt_eng_dset_clean[key]\n",
    "    ], axis=1)\n",
    "nusax_combined_eng_dset = datasets.DatasetDict(nusax_combined_eng_dset)\n",
    "nusax_combined_eng_dset = nusax_combined_eng_dset.map(\n",
    "    label2str, remove_columns=['label'], fn_kwargs={\"dset\": nusax_senti_dset['jav']['train']}\n",
    ").rename_columns({'str_label': 'label'})\n",
    "\n",
    "nusax_combined_ind_dset.save_to_disk(f'{save_path}/nusax_combined_ind_dset')\n",
    "nusax_combined_eng_dset.save_to_disk(f'{save_path}/nusax_combined_eng_dset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcab5b-ee4a-4993-9869-9488b2853d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####\n",
    "# AmericasNLI - XNLI Combined\n",
    "####\n",
    "save_path = '.'\n",
    "\n",
    "# Cannot use the XNLI from HuggingFace, somehow the results are not aligned,\n",
    "# so we use the original XNLI file (https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip) instead\n",
    "xnli_df = pd.read_csv('XNLI-1.0/xnli.dev.tsv', sep='\\t').reset_index()\n",
    "americasnli_combined_dset = {}\n",
    "for key in americasnli_dset.keys():\n",
    "    anli_df = pd.read_csv(f'https://github.com/abteen/americasnli/raw/main/data/anli_final/dev/{key}.tsv', sep='\\t')\n",
    "    anli_dset = datasets.Dataset.from_pandas(\n",
    "        anli_df[['premise', 'hypothesis', 'label']].rename({\n",
    "            'premise': 'premise_1', 'hypothesis': 'hypothesis_1'\n",
    "        }, axis='columns')\n",
    "    )    \n",
    "    \n",
    "    xnli_dset = datasets.Dataset.from_pandas(\n",
    "        xnli_df.loc[anli_df.id-1, ['sentence1', 'sentence2']].rename({\n",
    "            'sentence1': 'premise_2', 'sentence2': 'hypothesis_2'\n",
    "        }, axis='columns')\n",
    "    ).remove_columns('__index_level_0__')\n",
    "    americasnli_combined_dset[key] = datasets.concatenate_datasets([anli_dset, xnli_dset], axis=1)\n",
    "americasnli_combined_dset = datasets.DatasetDict(americasnli_combined_dset)\n",
    "americasnli_combined_dset.save_to_disk(f'{save_path}/americasnli_combined_dev_dset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec039f4-a7d9-4cf8-960a-dbeaebd57df1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####\n",
    "# MAFAND Random Label\n",
    "####\n",
    "random.seed(12345)\n",
    "save_path = '.'\n",
    "\n",
    "# Process MAFAND\n",
    "mafand_dset_clean = {}\n",
    "for key in mafand_dset.keys():\n",
    "    label_names = masakhanews_dset[key]['train'].features['label'].names\n",
    "    tmp_dset = {'text_1': [], 'text_2': [], 'label': []}\n",
    "    for split in mafand_dset[key].keys():        \n",
    "        for i in range(len(mafand_dset[key][split])):\n",
    "            tmp_dset['text_1'].append(mafand_dset[key][split][i]['translation'][key])\n",
    "            tmp_dset['text_2'].append(mafand_dset[key][split][i]['translation']['en'])\n",
    "            tmp_dset['label'].append(random.choice(label_names))\n",
    "    mafand_dset_clean[key] = datasets.Dataset.from_dict(tmp_dset)\n",
    "mafand_dset_clean = datasets.DatasetDict(mafand_dset_clean)\n",
    "mafand_dset_clean.save_to_disk(f'{save_path}/mafand_rand_label_dset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a226724c-ba00-424c-9a13-9198f9bd3806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_instruct_align)",
   "language": "python",
   "name": "env_instruct_align"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
